{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFne6kHRcUlxKqdecS28+O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahriarivari/NoisyBirdClassification/blob/main/pre_training_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# basic imports"
      ],
      "metadata": {
        "id": "SBz1K_RPw7qo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcUXp1UJwnM2"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from typing import Tuple, List\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from huggingface_hub import snapshot_download\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# downloading the dataset"
      ],
      "metadata": {
        "id": "S9Fa-k7Jw_Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# device agnostic code\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "zb6sSYObxCgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = \"RayanAi/Noisy_birds\"\n",
        "# Set the local directory where you want to store the dataset\n",
        "local_dataset_dir = \"./Noisy_birds\"  # You can change this path to your desired location\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(local_dataset_dir, exist_ok=True)\n",
        "\n",
        "# Suppress the output by redirecting it to os.devnull\n",
        "with open(os.devnull, 'w') as fnull:\n",
        "    # Save the original stdout\n",
        "    original_stdout = sys.stdout\n",
        "    try:\n",
        "        # Redirect stdout to devnull to suppress output\n",
        "        sys.stdout = fnull\n",
        "        # Download the dataset and store it locally\n",
        "        snapshot_download(repo_id=dataset_id, local_dir=local_dataset_dir, repo_type=\"dataset\")\n",
        "    finally:\n",
        "        # Restore the original stdout\n",
        "        sys.stdout = original_stdout\n",
        "\n",
        "# Print message when download is complete\n",
        "print(\"Dataset downloaded completely.\")\n",
        "\n",
        "# Calculate and print the total size of the downloaded files\n",
        "total_size = 0\n",
        "for dirpath, dirnames, filenames in os.walk(local_dataset_dir):\n",
        "    for f in filenames:\n",
        "        fp = os.path.join(dirpath, f)\n",
        "        total_size += os.path.getsize(fp)\n",
        "\n",
        "# Convert size to MB and print\n",
        "print(f\"Total size of downloaded files: {total_size / (1024 * 1024):.2f} MB\")\n",
        "\n",
        "# Get the absolute path of the dataset directory and print it\n",
        "dataset_abs_path = os.path.abspath(local_dataset_dir)\n",
        "print(f\"Dataset has been saved at: [{dataset_abs_path}]\")"
      ],
      "metadata": {
        "id": "aOqjs038xE3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qo ./Noisy_birds/Noisy_birds.zip -d ./Noisy_birds/"
      ],
      "metadata": {
        "id": "O-NqVT7YxXfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creatin dataset and datalaode objects"
      ],
      "metadata": {
        "id": "ouchlCzixY-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SimCLR Dataset that returns two views of each image\n",
        "class SimCLRDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, folder_path, transform):\n",
        "      self.folder_path = folder_path\n",
        "      self.image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path) if fname.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "      self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      img_path = self.image_paths[index]\n",
        "      img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "      # Apply the same transform twice to get two views of the image\n",
        "      img1 = self.transform(img)\n",
        "      img2 = self.transform(img)\n",
        "\n",
        "      return img1, img2\n",
        "\n",
        "# Define the data transformations (used to generate two views)\n",
        "simclr_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(64),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.4, 0.4, 0.4, 0.4),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Example image paths (modify according to your dataset)\n",
        "folder_path = 'path/to/your/images/folder'\n",
        "dataset = SimCLRDataset(folder_path, simclr_transform)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "eQncJsEsxfSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating the model"
      ],
      "metadata": {
        "id": "Y-hRCm_0ft8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    self.fc = nn.Linear(256 * 16 * 16, 128)  # Output of the CNN encoder\n",
        "\n",
        "    # Projection head for SimCLR (2-layer MLP)\n",
        "    self.projector = nn.Sequential(\n",
        "      nn.Linear(128, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(128, 64)  # Projection to a lower dimension (SimCLR uses a smaller space)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Forward pass through CNN encoder\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.pool(F.relu(self.conv3(x)))\n",
        "    x = x.view(x.size(0), -1)  # Flatten the output\n",
        "    x = self.fc(x)  # Pass through fully connected layer (output features)\n",
        "    x = F.normalize(x, dim=1)  # Normalize the features\n",
        "\n",
        "    # Apply projection head (used for SimCLR pretraining)\n",
        "    x = self.projector(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Ek-gWDh9fs7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define the loss function"
      ],
      "metadata": {
        "id": "iJ1ejEXBh79V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def info_nce_loss(features, batch_size, temperature, device):\n",
        "  # Create labels for positive pairs (augmented views of the same image)\n",
        "  labels = torch.cat([torch.arange(batch_size) for _ in range(2)], dim=0)\n",
        "  labels = (labels.unsqueeze(0) == labels.unsqueeze(1)).float().to(device)\n",
        "\n",
        "  # Normalize the features\n",
        "  features = F.normalize(features, dim=1)\n",
        "\n",
        "  # Compute the similarity matrix (dot product between all pairs of features)\n",
        "  similarity_matrix = torch.matmul(features, features.T)\n",
        "\n",
        "  # Remove self-similarity (diagonal elements)\n",
        "  mask = torch.eye(labels.shape[0], dtype=torch.bool).to(device)\n",
        "  labels = labels[~mask].view(labels.shape[0], -1)\n",
        "  similarity_matrix = similarity_matrix[~mask].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "  # Select positive and negative pairs\n",
        "  positives = similarity_matrix[labels.bool()].view(labels.shape[0], -1)\n",
        "  negatives = similarity_matrix[~labels.bool()].view(similarity_matrix.shape[0], -1)\n",
        "\n",
        "  # Concatenate positive and negative pairs\n",
        "  logits = torch.cat([positives, negatives], dim=1)\n",
        "  labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)\n",
        "\n",
        "  # Scale by temperature\n",
        "  logits /= temperature\n",
        "\n",
        "  return logits, labels\n"
      ],
      "metadata": {
        "id": "lizXEvCqh-4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training loop"
      ],
      "metadata": {
        "id": "DdbQt_oiiBdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize model, optimizer, and loss function\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "temperature = 0.5\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "\n",
        "  for img1, img2 in dataloader:\n",
        "    # Move data to the device (GPU or CPU)\n",
        "    img1, img2 = img1.to(device), img2.to(device)\n",
        "\n",
        "    # Pass both views through the encoder and projection head\n",
        "    features_img1 = model(img1)\n",
        "    features_img2 = model(img2)\n",
        "\n",
        "    # Concatenate features from both views\n",
        "    features = torch.cat([features_img1, features_img2], dim=0)\n",
        "\n",
        "    # Compute the InfoNCE loss\n",
        "    logits, labels = info_nce_loss(features, batch_size=batch_size, temperature=temperature, device=device)\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    # Backpropagation and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n"
      ],
      "metadata": {
        "id": "cqW11YRXiC7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}